import pandas as pd
import numpy as np
import ast
import torch
from flask import Flask, request, jsonify
from pyngrok import ngrok
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from deepctr_torch.inputs import SparseFeat, DenseFeat, VarLenSparseFeat
from deepctr_torch.models import DeepFM

class ServiceConfig:
    CSV_PATH = '../data/steam/deepfm_train_100k.csv'
    MODEL_PATH = 'deepfm_steam_weights.pth'
    MAX_TAG_LEN = 5
    EMBEDDING_DIM = 32
    DNN_HIDDEN_UNITS = (128, 64)
    DNN_DROPOUT = 0.5
    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
    PORT = 5000
    NGROK_TOKEN = "è¿™é‡Œç²˜è´´ä½ çš„_Ngrok_Token"

cfg = ServiceConfig()

def pad_sequences(sequences, maxlen, value=0):
    result = np.full((len(sequences), maxlen), value, dtype=np.int32)
    for i, seq in enumerate(sequences):
        if len(seq) > 0:
            trunc = seq[:maxlen]
            result[i, :len(trunc)] = trunc
    return result

# å…¨å±€ä¿å­˜ user encoderï¼Œç”¨äºæŠŠ "Hardcore_FPS" è½¬æˆæ•°å­—
global_user_lbe = None

def load_data_struct(csv_path, config):
    print(f"ğŸ“‚ [Service] è¯»å–æ•°æ®ç´¢å¼•: {csv_path} ...")
    try:
        data = pd.read_csv(csv_path)
    except FileNotFoundError: return None, None, None

    data['tags_list'] = data['tags_list'].apply(lambda x: ast.literal_eval(x))
    
    # 1. Tag
    all_tags = [tag for sublist in data['tags_list'] for tag in sublist]
    tag_lbe = LabelEncoder()
    tag_lbe.fit(all_tags)
    max_tag_id = len(tag_lbe.classes_) + 1
    data['tags_list_idx'] = data['tags_list'].apply(lambda x: [i+1 for i in tag_lbe.transform(x)] if len(x)>0 else [])
    
    # 2. Item
    item_lbe = LabelEncoder()
    data['item_id_idx'] = item_lbe.fit_transform(data['item_id'])
    max_item_id = data['item_id_idx'].max() + 1
    
    # 3. UserType (ğŸ”¥ å…³é”®ï¼šä¿å­˜è¿™ä¸ª encoder)
    global global_user_lbe
    global_user_lbe = LabelEncoder()
    # å¿…é¡» fit æ‰€æœ‰çš„ç±»å‹ï¼Œä¿è¯å’Œè®­ç»ƒæ—¶ ID ä¸€è‡´
    data['user_type_idx'] = global_user_lbe.fit_transform(data['user_type'])
    max_user_type_id = data['user_type_idx'].max() + 1
    print(f"ğŸ”¥ æ”¯æŒçš„ç©å®¶ç±»å‹: {list(global_user_lbe.classes_)}")
    
    # 4. Price
    mms = MinMaxScaler(feature_range=(0, 1))
    data['price_norm'] = mms.fit_transform(data[['price']])
    
    fixlen_feature_columns = [
        SparseFeat('item_id_idx', vocabulary_size=max_item_id, embedding_dim=config.EMBEDDING_DIM),
        SparseFeat('user_type_idx', vocabulary_size=max_user_type_id, embedding_dim=config.EMBEDDING_DIM), # ğŸ”¥
        DenseFeat('price_norm', dimension=1)
    ]
    varlen_feature_columns = [
        VarLenSparseFeat(
            SparseFeat('tags', vocabulary_size=max_tag_id, embedding_dim=config.EMBEDDING_DIM),
            maxlen=config.MAX_TAG_LEN, combiner='mean', length_name=None
        )
    ]
    return fixlen_feature_columns + varlen_feature_columns, fixlen_feature_columns + varlen_feature_columns, data

app = Flask(__name__)
model_steam = None
full_data_df = None

def init_model():
    global model_steam, full_data_df
    linear_cols, dnn_cols, df = load_data_struct(cfg.CSV_PATH, cfg)
    if linear_cols is None: return
    full_data_df = df
    
    model_steam = DeepFM(linear_feature_columns=linear_cols, dnn_feature_columns=dnn_cols, task='binary', 
                         dnn_hidden_units=cfg.DNN_HIDDEN_UNITS, dnn_dropout=cfg.DNN_DROPOUT, device=cfg.DEVICE)
    try:
        model_steam.load_state_dict(torch.load(cfg.MODEL_PATH, map_location=cfg.DEVICE))
        model_steam.eval()
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    except Exception as e: print(f"âŒ é”™è¯¯: {e}")

@app.route('/recommend', methods=['POST'])
def recommend():
    if model_steam is None: return jsonify({"error": "Model not loaded"}), 500
    try:
        req_json = request.json
        top_k = req_json.get('top_k', 3)
        # è·å–è¯·æ±‚çš„ç©å®¶ç±»å‹ï¼Œé»˜è®¤ Hardcore_FPS
        user_type_str = req_json.get('type', 'Hardcore_FPS') 
        
        # ğŸ”¥ 1. å°†ç±»å‹å­—ç¬¦ä¸²è½¬ä¸º ID
        if user_type_str not in global_user_lbe.classes_:
            return jsonify({"error": f"Unknown type: {user_type_str}. Supported: {list(global_user_lbe.classes_)}"}), 400
        
        # æ‹¿åˆ°å¯¹åº”çš„æ•°å­— ID (ä¾‹å¦‚ 2)
        user_type_id = global_user_lbe.transform([user_type_str])[0]
        
        print(f"ğŸ® æ”¶åˆ°è¯·æ±‚: Type={user_type_str}(ID={user_type_id}), Top {top_k}")

        # ğŸ”¥ 2. æ„é€ å…¨é‡è¾“å…¥ (å…³é”®æ­¥éª¤)
        # æˆ‘ä»¬æœ‰ N ä¸ªæ¸¸æˆï¼Œéœ€è¦æ„é€  N ä¸ª user_type_id
        # ä¹Ÿå°±æ˜¯ï¼š[2, 2, 2, ..., 2] (é•¿åº¦ç­‰äºæ¸¸æˆæ•°é‡)
        # æ„æ€æ˜¯ï¼šé¢„æµ‹â€œè¿™ä¸ªç‰¹å®šçš„ç©å®¶â€å¯¹â€œæ¯ä¸€ä¸ªæ¸¸æˆâ€çš„å–œå¥½
        num_items = len(full_data_df)
        user_type_col = np.full(num_items, user_type_id) 

        tags_padded = pad_sequences(list(full_data_df['tags_list_idx']), maxlen=cfg.MAX_TAG_LEN)
        
        model_input = {
            'item_id_idx': full_data_df['item_id_idx'].values,
            'user_type_idx': user_type_col,  # ğŸ”¥ è¿™é‡Œä¼ å…¥çš„æ˜¯å…¨é‡çš„å•ä¸€ç”¨æˆ·ID
            'price_norm': full_data_df['price_norm'].values,
            'tags': tags_padded
        }
        
        with torch.no_grad():
            scores = model_steam.predict(model_input, batch_size=4096)
            
        res_df = full_data_df.copy()
        res_df['score'] = scores
        # æ’åºå¹¶å»é‡
        top_items = res_df.sort_values(by='score', ascending=False).drop_duplicates(subset=['item_id']).head(top_k)
        
        results = [{
            "id": str(r['item_id']),
            "title": r['title'],
            "score": float(r['score']),
            "cover": r.get('cover_url', ''),
            "tags": r.get('tag_names', '')
        } for _, r in top_items.iterrows()]
        
        return jsonify({"code": 200, "type": user_type_str, "data": results})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    init_model()
    if not cfg.NGROK_TOKEN.startswith("è¿™é‡Œ"):
        ngrok.set_auth_token(cfg.NGROK_TOKEN)
        ngrok.kill()
        try: print(f"ğŸŒ {ngrok.connect(cfg.PORT, bind_tls=True).public_url}/recommend")
        except: pass
    app.run(port=cfg.PORT, use_reloader=False)