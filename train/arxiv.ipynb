{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfde963d",
   "metadata": {},
   "source": [
    "## ArXiv æ•°æ®åŠ è½½ä¸é¢„å¤„ç†\n",
    "\n",
    "æ ¸å¿ƒé€»è¾‘ï¼š\n",
    "\n",
    "1. å¤„ç† abstract + title: ä½¿ç”¨ sentence-transformers å®æ—¶ç”Ÿæˆ 384ç»´ è¯­ä¹‰å‘é‡ã€‚\n",
    "\n",
    "2. å¤„ç† category: ç®€å•çš„ Label Encodingã€‚\n",
    "\n",
    "3. é‡ç‚¹ï¼šå°† 384ç»´ å‘é‡æ‹†è§£ä¸º 384 ä¸ª DenseFeat å–‚ç»™æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9da26778-94d3-4364-9cd7-e4806c20700f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T13:40:26.696612Z",
     "iopub.status.busy": "2025-12-15T13:40:26.696415Z",
     "iopub.status.idle": "2025-12-15T13:40:51.545292Z",
     "shell.execute_reply": "2025-12-15T13:40:51.544696Z",
     "shell.execute_reply.started": "2025-12-15T13:40:26.696596Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.cloud.aliyuncs.com/pypi/simple\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/site-packages (2.9.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/site-packages (2.3.5)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/site-packages (1.7.2)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/site-packages (5.1.2)\n",
      "Collecting deepctr-torch\n",
      "  Using cached https://mirrors.cloud.aliyuncs.com/pypi/packages/95/1c/5620aafbbaff9ad92c0135924b970cadd59453d9a653b7420eb485a1a1c7/deepctr_torch-0.2.9-py3-none-any.whl (82 kB)\n",
      "Collecting flask\n",
      "  Using cached https://mirrors.cloud.aliyuncs.com/pypi/packages/ec/f9/7f9263c5695f4bd0023734af91bedb2ff8209e8de6ead162f35d8dc762fd/flask-3.1.2-py3-none-any.whl (103 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (2.32.5)\n",
      "Collecting pyngrok\n",
      "  Using cached https://mirrors.cloud.aliyuncs.com/pypi/packages/bf/60/ab93dfb5721ba65f36bb6c2e07adfa21a8ac6525760b60f2ad6d31f7d098/pyngrok-7.5.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.11/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.11/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.11/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.11/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.11/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.11/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.11/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.11/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.11/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.11/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.11/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.11/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.11/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /usr/local/lib/python3.11/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/site-packages (from sentence-transformers) (4.57.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/site-packages (from sentence-transformers) (12.0.0)\n",
      "Collecting tensorflow (from deepctr-torch)\n",
      "  Using cached https://mirrors.cloud.aliyuncs.com/pypi/packages/ff/0c/7df285ee8a88139fab0b237003634d90690759fae9c18f55ddb7c04656ec/tensorflow-2.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.6 MB)\n",
      "Collecting blinker>=1.9.0 (from flask)\n",
      "  Using cached https://mirrors.cloud.aliyuncs.com/pypi/packages/10/cb/f2ad4230dc2eb1a74edf38f1a38b9b52277f75bef262d8908e60d957e13c/blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/site-packages (from flask) (8.2.1)\n",
      "Collecting itsdangerous>=2.2.0 (from flask)\n",
      "  Using cached https://mirrors.cloud.aliyuncs.com/pypi/packages/04/96/92447566d16df59b2a776c0fb82dbc4d9e07cd95062562af01e408583fc4/itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/site-packages (from flask) (3.0.3)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/site-packages (from flask) (3.1.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests) (2025.11.12)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/site-packages (from pyngrok) (6.0.3)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/site-packages (from tensorflow->deepctr-torch) (2.3.1)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow->deepctr-torch)\n",
      "  Using cached https://mirrors.cloud.aliyuncs.com/pypi/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow->deepctr-torch)\n",
      "  Using cached https://mirrors.cloud.aliyuncs.com/pypi/packages/ee/1b/00a78aa2e8fbd63f9af08c9c19e6deb3d5d66b4dda677a0f61654680ee89/flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow->deepctr-torch)\n",
      "  Using cached https://mirrors.cloud.aliyuncs.com/pypi/packages/1d/33/f1c6a276de27b7d7339a34749cc33fa87f077f921969c47185d34a887ae2/gast-0.7.0-py3-none-any.whl (22 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow->deepctr-torch)\n",
      "  Using cached https://mirrors.cloud.aliyuncs.com/pypi/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow->deepctr-torch)\n",
      "  Using cached https://mirrors.cloud.aliyuncs.com/pypi/packages/1d/fc/716c1e62e512ef1c160e7984a73a5fc7df45166f2ff3f254e71c58076f7c/libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow->deepctr-torch)\n",
      "  Using cached https://mirrors.cloud.aliyuncs.com/pypi/packages/23/cd/066e86230ae37ed0be70aae89aabf03ca8d9f39c8aea0dec8029455b5540/opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.11/site-packages (from tensorflow->deepctr-torch) (6.33.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/site-packages (from tensorflow->deepctr-torch) (65.5.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/site-packages (from tensorflow->deepctr-torch) (3.2.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow->deepctr-torch)\n",
      "  Using cached https://mirrors.cloud.aliyuncs.com/pypi/packages/5b/36/825b44c8a10556957bc0c1d84c7b29a40e05fcf1873b6c40aa9dbe0bd972/wrapt-2.0.1-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (114 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/site-packages (from tensorflow->deepctr-torch) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in /usr/local/lib/python3.11/site-packages (from tensorflow->deepctr-torch) (2.20.0)\n",
      "Collecting keras>=3.10.0 (from tensorflow->deepctr-torch)\n",
      "  Using cached https://mirrors.cloud.aliyuncs.com/pypi/packages/ba/61/cc8be27bd65082440754be443b17b6f7c185dec5e00dfdaeab4f8662e4a8/keras-3.12.0-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/site-packages (from tensorflow->deepctr-torch) (3.15.1)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow->deepctr-torch)\n",
      "  Using cached https://mirrors.cloud.aliyuncs.com/pypi/packages/a9/80/19189ea605017473660e43762dc853d2797984b3c7bf30ce656099add30c/ml_dtypes-0.5.4-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow->deepctr-torch) (0.45.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/site-packages (from keras>=3.10.0->tensorflow->deepctr-torch) (14.2.0)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow->deepctr-torch)\n",
      "  Using cached https://mirrors.cloud.aliyuncs.com/pypi/packages/b2/bc/465daf1de06409cdd4532082806770ee0d8d7df434da79c76564d0f69741/namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow->deepctr-torch)\n",
      "  Using cached https://mirrors.cloud.aliyuncs.com/pypi/packages/2a/6f/7f2238ec5e9d33e56252c30880bb8f44aec1415474b62b9e33b38594953d/optree-0.18.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (400 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/site-packages (from tensorboard~=2.20.0->tensorflow->deepctr-torch) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/site-packages (from tensorboard~=2.20.0->tensorflow->deepctr-torch) (0.7.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/site-packages (from rich->keras>=3.10.0->tensorflow->deepctr-torch) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/site-packages (from rich->keras>=3.10.0->tensorflow->deepctr-torch) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow->deepctr-torch) (0.1.2)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, pyngrok, optree, opt_einsum, ml_dtypes, itsdangerous, google_pasta, gast, blinker, astunparse, flask, keras, tensorflow, deepctr-torch\n",
      "Successfully installed astunparse-1.6.3 blinker-1.9.0 deepctr-torch-0.2.9 flask-3.1.2 flatbuffers-25.9.23 gast-0.7.0 google_pasta-0.2.0 itsdangerous-2.2.0 keras-3.12.0 libclang-18.1.1 ml_dtypes-0.5.4 namex-0.1.0 opt_einsum-3.4.0 optree-0.18.0 pyngrok-7.5.0 tensorflow-2.20.0 wrapt-2.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“„ å®‰è£… ArXiv æ¨èç³»ç»Ÿæ‰€éœ€çš„åŒ…\n",
    "!pip install torch pandas numpy scikit-learn sentence-transformers deepctr-torch flask requests pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a6cfc44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T13:54:02.454582Z",
     "iopub.status.busy": "2025-12-15T13:54:02.454357Z",
     "iopub.status.idle": "2025-12-15T13:54:04.042733Z",
     "shell.execute_reply": "2025-12-15T13:54:04.041967Z",
     "shell.execute_reply.started": "2025-12-15T13:54:02.454563Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/activations_tf.py:22\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf_keras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# å°è¯•å¯¼å…¥ sentence_transformersï¼Œå¦‚æœæ²¡æœ‰åˆ™ç”¨éšæœºå‘é‡ä»£æ›¿ï¼ˆé˜²æ­¢æŠ¥é”™ï¼‰\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m     12\u001b[39m     HAS_BERT = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/sentence_transformers/__init__.py:15\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     11\u001b[39m     export_dynamic_quantized_onnx_model,\n\u001b[32m     12\u001b[39m     export_optimized_onnx_model,\n\u001b[32m     13\u001b[39m     export_static_quantized_openvino_model,\n\u001b[32m     14\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcross_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     CrossEncoder,\n\u001b[32m     17\u001b[39m     CrossEncoderModelCardData,\n\u001b[32m     18\u001b[39m     CrossEncoderTrainer,\n\u001b[32m     19\u001b[39m     CrossEncoderTrainingArguments,\n\u001b[32m     20\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mLoggingHandler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/sentence_transformers/cross_encoder/__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mCrossEncoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_card\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderModelCardData\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:29\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_onnx_model, load_openvino_model\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcross_encoder\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfit_mixin\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FitMixin\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcross_encoder\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_card\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderModelCardData, generate_model_card\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcross_encoder\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     32\u001b[39m     cross_encoder_init_args_decorator,\n\u001b[32m     33\u001b[39m     cross_encoder_predict_rank_args_decorator,\n\u001b[32m     34\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/sentence_transformers/cross_encoder/fit_mixin.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcross_encoder\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtraining_args\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainingArguments\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mNoDuplicatesDataLoader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NoDuplicatesDataLoader\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mSentenceLabelDataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceLabelDataset\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mSentenceEvaluator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceEvaluator\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InputExample\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/sentence_transformers/datasets/__init__.py:13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mDenoisingAutoEncoderDataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DenoisingAutoEncoderDataset\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mNoDuplicatesDataLoader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NoDuplicatesDataLoader\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mParallelSentencesDataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mSentenceLabelDataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceLabelDataset\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mSentencesDataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentencesDataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/sentence_transformers/datasets/ParallelSentencesDataset.py:19\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InputExample\n\u001b[32m     22\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:36\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdynamic_module_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_class_from_dynamic_module, get_relative_import_files\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_card\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformerModelCardData, generate_model_card\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Router\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mModule\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Module\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/sentence_transformers/model_card.py:26\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautonotebook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrainerCallback\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CodeCarbonCallback\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodelcard\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_markdown_table\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer_callback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrainerControl, TrainerState\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1229\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/utils/import_utils.py:2317\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module:\n\u001b[32m   2316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2317\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2318\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2319\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/utils/import_utils.py:2347\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/utils/import_utils.py:2345\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2344\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2345\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/importlib/__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    124\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/integrations/integration_utils.py:60\u001b[39m\n\u001b[32m     57\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TFPreTrainedModel\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1229\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/utils/import_utils.py:2317\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module:\n\u001b[32m   2316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2317\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2318\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2319\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/utils/import_utils.py:2347\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/utils/import_utils.py:2345\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2344\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2345\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/importlib/__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    124\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:38\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackaging\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataCollatorWithPadding, DefaultDataCollator\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations_tf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdynamic_module_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m custom_object_save\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/transformers/activations_tf.py:27\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras.__version__).major > \u001b[32m2\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     28\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     30\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`pip install tf-keras`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m         )\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_gelu\u001b[39m(x):\n\u001b[32m     35\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[33;03m    Gaussian Error Linear Unit. Original Implementation of the gelu activation function in Google Bert repo when\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33;03m    initially created. For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[33;03m    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) Also see\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[33;03m    https://huggingface.co/papers/1606.08415\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# ğŸ“„ ArXiv æ•°æ®åŠ è½½å™¨ (å«æ–‡æœ¬å‘é‡åŒ–)\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from deepctr_torch.inputs import SparseFeat, DenseFeat\n",
    "\n",
    "# å°è¯•å¯¼å…¥ sentence_transformersï¼Œå¦‚æœæ²¡æœ‰åˆ™ç”¨éšæœºå‘é‡ä»£æ›¿ï¼ˆé˜²æ­¢æŠ¥é”™ï¼‰\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    HAS_BERT = True\n",
    "except ImportError:\n",
    "    HAS_BERT = False\n",
    "    print(\"âš ï¸ æœªå®‰è£… sentence-transformersï¼Œå°†ä½¿ç”¨éšæœºå‘é‡æ¨¡æ‹Ÿæ•°æ®ã€‚\")\n",
    "\n",
    "def load_arxiv_data(csv_path):\n",
    "    print(f\"Loading ArXiv data from: {csv_path} ...\")\n",
    "    data = pd.read_csv(csv_path)\n",
    "    \n",
    "    # 1. åŸºç¡€ç‰¹å¾ç¼–ç \n",
    "    # Item ID (String -> Int)\n",
    "    lbe_id = LabelEncoder()\n",
    "    data['item_id_idx'] = lbe_id.fit_transform(data['item_id'])\n",
    "    \n",
    "    # Category (cs.CV -> 0, cs.RO -> 1)\n",
    "    lbe_cat = LabelEncoder()\n",
    "    data['category_idx'] = lbe_cat.fit_transform(data['category'])\n",
    "    \n",
    "    # 2. æ–‡æœ¬å‘é‡åŒ– (Text Embedding)\n",
    "    # æˆ‘ä»¬ä½¿ç”¨ Title + Abstract æ¥ç”Ÿæˆè¯­ä¹‰å‘é‡\n",
    "    EMBEDDING_DIM = 384 # MiniLM çš„æ ‡å‡†ç»´åº¦\n",
    "    \n",
    "    if HAS_BERT:\n",
    "        print(\"â³ æ­£åœ¨è°ƒç”¨ BERT æ¨¡å‹ç”Ÿæˆæ‘˜è¦å‘é‡ (è¿™å¯èƒ½éœ€è¦å‡ ç§’é’Ÿ)...\")\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        # æ‹¼æ¥ Title å’Œ Abstract å¢åŠ è¯­ä¹‰ä¸°å¯Œåº¦\n",
    "        sentences = (data['title'] + \". \" + data['abstract']).tolist()\n",
    "        text_vectors = model.encode(sentences) # Shape: (N, 384)\n",
    "    else:\n",
    "        # æ¨¡æ‹Ÿæ•°æ® (ä»…è°ƒè¯•ç”¨)\n",
    "        text_vectors = np.random.random((len(data), EMBEDDING_DIM)).astype(np.float32)\n",
    "\n",
    "    # 3. å®šä¹‰ç‰¹å¾é…ç½®\n",
    "    # Sparse éƒ¨åˆ†: ID å’Œ Category\n",
    "    feature_columns = [\n",
    "        SparseFeat(name='item_id_idx', vocabulary_size=data['item_id_idx'].max() + 1, embedding_dim=16),\n",
    "        SparseFeat(name='category_idx', vocabulary_size=data['category_idx'].max() + 1, embedding_dim=16)\n",
    "    ]\n",
    "    \n",
    "    # Dense éƒ¨åˆ†: æŠŠ 384 ç»´å‘é‡æ‹†æˆ 384 ä¸ª DenseFeat\n",
    "    # è¿™æ˜¯è®© DeepFM å¤„ç† BERT å‘é‡çš„æ ‡å‡†åšæ³•\n",
    "    for i in range(EMBEDDING_DIM):\n",
    "        feature_columns.append(DenseFeat(name=f'v_{i}', dimension=1))\n",
    "        \n",
    "    dnn_feature_columns = feature_columns\n",
    "    linear_feature_columns = feature_columns\n",
    "\n",
    "    # 4. ç»„è£…è¾“å…¥å­—å…¸\n",
    "    model_input = {\n",
    "        'item_id_idx': data['item_id_idx'].values,\n",
    "        'category_idx': data['category_idx'].values\n",
    "    }\n",
    "    \n",
    "    # æŠŠå‘é‡æ‹†å¼€å¡è¿›å­—å…¸ (v_0, v_1 ... v_383)\n",
    "    for i in range(EMBEDDING_DIM):\n",
    "        model_input[f'v_{i}'] = text_vectors[:, i]\n",
    "        \n",
    "    # Label\n",
    "    target = data['label'].values\n",
    "    \n",
    "    return model_input, linear_feature_columns, dnn_feature_columns, target\n",
    "\n",
    "# --- æµ‹è¯•è¿è¡Œ ---\n",
    "# arxiv_input, arxiv_linear, arxiv_dnn, arxiv_y = load_arxiv_data('arxiv_data.csv')\n",
    "# print(\"ArXiv Input Keys Count:\", len(arxiv_input)) # åº”è¯¥æ˜¯ 2 + 384 = 386 ä¸ª key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffce771",
   "metadata": {},
   "source": [
    "## ArXiv æ¨¡å‹è®­ç»ƒ (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea23499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from deepctr_torch.models import DeepFM\n",
    "from deepctr_torch.callbacks import EarlyStopping\n",
    "\n",
    "# 1. å‡†å¤‡æ•°æ®\n",
    "# ç¡®ä¿ä½ å·²ç»è¿è¡Œäº† Cell 2 å®šä¹‰äº† load_arxiv_data\n",
    "arxiv_csv_path = '../data/arxiv/train_arxiv_Researcher_LLM.csv'\n",
    "arxiv_vec_path = 'arxiv_vectors.npy'\n",
    "input_dict_arxiv, linear_cols_arxiv, dnn_cols_arxiv, target_arxiv = load_arxiv_data(arxiv_csv_path)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 2. åˆå§‹åŒ– DeepFM æ¨¡å‹\n",
    "# ArXiv çš„ç‰¹å¾ç»´åº¦å¾ˆå¤§ (384ç»´)ï¼Œå»ºè®®æŠŠ DNN å±‚è®¾å®½ä¸€ç‚¹\n",
    "model_arxiv = DeepFM(linear_feature_columns=linear_cols_arxiv, \n",
    "                     dnn_feature_columns=dnn_cols_arxiv, \n",
    "                     task='binary', \n",
    "                     dnn_hidden_units=(256, 128), \n",
    "                     device=device)\n",
    "\n",
    "# 3. ç¼–è¯‘ä¸è®­ç»ƒ\n",
    "model_arxiv.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy', 'auc'])\n",
    "\n",
    "history_arxiv = model_arxiv.fit(input_dict_arxiv, target_arxiv, \n",
    "                                batch_size=32, \n",
    "                                epochs=10, \n",
    "                                verbose=2, \n",
    "                                validation_split=0.2)\n",
    "\n",
    "print(\"ğŸ‰ ArXiv æ¨¡å‹è®­ç»ƒå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ed187f",
   "metadata": {},
   "source": [
    "## æ¨¡å‹ä¿å­˜ (Model Saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ddfae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# ğŸ’¾ æ¨¡å‹æƒé‡ä¿å­˜\n",
    "# ==========================================\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# 1. å®šä¹‰ä¿å­˜è·¯å¾„ (å»ºè®®æ”¾åœ¨å½“å‰å·¥ä½œç›®å½•)\n",
    "arxiv_model_path = 'deepfm_arxiv_weights.pth'\n",
    "\n",
    "# 2. ä¿å­˜ ArXiv æ¨¡å‹\n",
    "if 'model_arxiv' in locals():\n",
    "    torch.save(model_arxiv.state_dict(), arxiv_model_path)\n",
    "    print(f\"âœ… ArXiv æ¨¡å‹æƒé‡å·²ä¿å­˜è‡³: {arxiv_model_path}\")\n",
    "else:\n",
    "    print(\"âš ï¸ æœªæ£€æµ‹åˆ° model_arxivï¼Œè·³è¿‡ä¿å­˜ã€‚\")\n",
    "\n",
    "print(\"\\næç¤º: åœ¨ ModelScope å·¦ä¾§æ–‡ä»¶åˆ—è¡¨ä¸­åº”è¯¥èƒ½çœ‹åˆ°è¿™ä¸¤ä¸ª .pth æ–‡ä»¶äº†ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2f126",
   "metadata": {},
   "source": [
    "## åŠ è½½æƒé‡å¹¶å¯åŠ¨ API æœåŠ¡ (Loading & Serving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333381fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# ğŸš€ æ¨¡å‹åŠ è½½ä¸ API æœåŠ¡å¯åŠ¨\n",
    "# ==========================================\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from deepctr_torch.models import DeepFM\n",
    "from flask import Flask, request, jsonify\n",
    "import threading\n",
    "\n",
    "# 1. é‡æ–°å®šä¹‰ä¸€äº›åŸºç¡€é…ç½® (é˜²æ­¢é‡å¯å†…æ ¸åå˜é‡ä¸¢å¤±)\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ARXIV_CSV = '../data/arxiv/train_arxiv_Researcher_LLM.csv'  # ç¡®ä¿æ–‡ä»¶åå¯¹\n",
    "ARXIV_VEC = 'arxiv_vectors.npy'\n",
    "\n",
    "# 2. å®šä¹‰åŠ è½½å‡½æ•° (å¤ç”¨ä¹‹å‰çš„é€»è¾‘ï¼Œåªä¸ºäº†è·å–ç‰¹å¾åˆ—é…ç½®æ¥åˆå§‹åŒ–æ¨¡å‹)\n",
    "#    æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬éœ€è¦ import ä¹‹å‰å®šä¹‰çš„ load_arxiv_data\n",
    "#    å¦‚æœä½ é‡å¯äº†å†…æ ¸ï¼Œè¯·ç¡®ä¿æŠŠâ€œæ•°æ®é¢„å¤„ç†â€é‚£ä¸ª Cell å…ˆè¿è¡Œä¸€éï¼Œè®©å‡½æ•°ç”Ÿæ•ˆã€‚\n",
    "\n",
    "print(\"ğŸ”„ æ­£åœ¨åˆå§‹åŒ–æœåŠ¡ï¼Œé‡æ–°æ„å»ºæ¨¡å‹ç»“æ„...\")\n",
    "\n",
    "# --- B. åŠ è½½ ArXiv æ¨¡å‹ ---\n",
    "arxiv_input, arxiv_linear, arxiv_dnn, _ = load_arxiv_data(ARXIV_CSV, ARXIV_VEC)\n",
    "service_model_arxiv = DeepFM(arxiv_linear, arxiv_dnn, task='binary', dnn_hidden_units=(256, 128), device=DEVICE)\n",
    "try:\n",
    "    service_model_arxiv.load_state_dict(torch.load('deepfm_arxiv_weights.pth', map_location=DEVICE))\n",
    "    service_model_arxiv.eval()\n",
    "    print(\"âœ… ArXiv æ¨¡å‹åŠ è½½æˆåŠŸï¼\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ æœªæ‰¾åˆ° ArXiv æƒé‡æ–‡ä»¶ã€‚\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. æ­å»º Flask API\n",
    "# ==========================================\n",
    "app = Flask(__name__)\n",
    "\n",
    "# å®šä¹‰æ¨èé€»è¾‘ (å°è£…å¥½çš„å·¥å…·å‡½æ•°)\n",
    "def get_recommendation(model, input_data, csv_path, top_k=3):\n",
    "    # 1. é¢„æµ‹\n",
    "    pred_ans = model.predict(input_data, batch_size=256)\n",
    "    # 2. è¯»åŸå§‹æ–‡ä»¶ç”¨äºå±•ç¤º\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['score'] = pred_ans\n",
    "    # 3. æ’åº\n",
    "    top_items = df.sort_values(by='score', ascending=False).head(top_k)\n",
    "    # 4. æ ¼å¼åŒ–è¾“å‡º\n",
    "    results = []\n",
    "    for _, row in top_items.iterrows():\n",
    "        results.append({\n",
    "            \"id\": str(row['item_id']),\n",
    "            \"title\": row['title'],\n",
    "            \"score\": float(row['score']),\n",
    "            \"cover\": row.get('cover_url', row.get('pdf_url', '')),\n",
    "            \"tags\": row.get('tag_names', row.get('category', ''))\n",
    "        })\n",
    "    return results\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    return jsonify({\"status\": \"ok\", \"device\": DEVICE})\n",
    "\n",
    "@app.route('/recommend', methods=['POST'])\n",
    "def recommend():\n",
    "    \"\"\"\n",
    "    API æ¥å£\n",
    "    Input JSON: { \"type\": \"arxiv\", \"top_k\": 3 }\n",
    "    \"\"\"\n",
    "    data = request.json\n",
    "    rec_type = data.get('type', 'arxiv')\n",
    "    top_k = data.get('top_k', 3)\n",
    "    \n",
    "    try:\n",
    "        if rec_type == 'arxiv':\n",
    "            res = get_recommendation(service_model_arxiv, arxiv_input, ARXIV_CSV, top_k)\n",
    "        else:\n",
    "            return jsonify({\"error\": \"Unknown type\"}), 400\n",
    "            \n",
    "        return jsonify({\n",
    "            \"code\": 200,\n",
    "            \"type\": rec_type,\n",
    "            \"data\": res\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return jsonify({\"code\": 500, \"error\": str(e)})\n",
    "\n",
    "# ==========================================\n",
    "# 4. å¯åŠ¨æœåŠ¡ (åœ¨ Notebook ä¸­åå°è¿è¡Œ)\n",
    "# ==========================================\n",
    "def run_app():\n",
    "    # ç«¯å£è®¾ç½®ä¸º 6006 æˆ– 5000\n",
    "    app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)\n",
    "\n",
    "# ä½¿ç”¨çº¿ç¨‹å¯åŠ¨ï¼Œé¿å…é˜»å¡ Notebook çš„ä¸»è¿›ç¨‹ï¼Œè¿™æ ·ä½ è¿˜èƒ½ç»§ç»­è·‘ä¸‹é¢çš„æµ‹è¯•ä»£ç \n",
    "t = threading.Thread(target=run_app)\n",
    "t.daemon = True\n",
    "t.start()\n",
    "\n",
    "print(\"ğŸš€ æœåŠ¡å·²å¯åŠ¨ï¼ç›‘å¬ç«¯å£ 5000...\")\n",
    "print(\"ä½ å¯ä»¥è¿è¡Œä¸‹ä¸€ä¸ª Cell æ¥æµ‹è¯•æ¥å£ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38303ced",
   "metadata": {},
   "source": [
    "## æµ‹è¯•æœåŠ¡ (Client Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55aaef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# ğŸ§ª æ¥å£æµ‹è¯• (æ¨¡æ‹Ÿ Dify è°ƒç”¨)\n",
    "# ==========================================\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# ç­‰å¾…ä¸€ç§’è®©æœåŠ¡å®Œå…¨å¯åŠ¨\n",
    "time.sleep(2)\n",
    "\n",
    "url = \"http://127.0.0.1:5000/recommend\"\n",
    "\n",
    "# 2. æµ‹è¯• ArXiv æ¨è\n",
    "print(\"ğŸ“¡ è¯·æ±‚ ArXiv æ¨è...\")\n",
    "payload_arxiv = {\"type\": \"arxiv\", \"top_k\": 2}\n",
    "try:\n",
    "    resp = requests.post(url, json=payload_arxiv)\n",
    "    print(\"ArXiv Response:\", resp.json())\n",
    "except Exception as e:\n",
    "    print(\"ArXiv Request Failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6e21a5",
   "metadata": {},
   "source": [
    "## æœ€ç»ˆæœåŠ¡ä»£ç  Cell (ç›´æ¥è¿è¡Œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bdfca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# ğŸ“„ [Cell B] ArXiv DeepFM Server (Port 5001)\n",
    "# ==========================================\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from deepctr_torch.models import DeepFM\n",
    "from flask import Flask, request, jsonify\n",
    "from pyngrok import ngrok\n",
    "import threading\n",
    "\n",
    "# âš ï¸âš ï¸âš ï¸ å¡«å…¥ä½ çš„ Ngrok Token âš ï¸âš ï¸âš ï¸\n",
    "NGROK_TOKEN = \"è¿™é‡Œç²˜è´´ä½ çš„_Ngrok_Token\" \n",
    "PORT = 5001 # ArXiv æœåŠ¡è¿è¡Œåœ¨ 5001 ç«¯å£ (é˜²æ­¢å’Œ Steam å†²çª)\n",
    "\n",
    "# ================= 1. åŠ è½½ ArXiv æ¨¡å‹ =================\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ARXIV_CSV = 'train_arxiv_Researcher_LLM.csv'\n",
    "ARXIV_VEC = 'arxiv_vectors.npy'\n",
    "\n",
    "print(\"ğŸ”„ æ­£åœ¨åˆå§‹åŒ– ArXiv æ¨¡å‹...\")\n",
    "\n",
    "try:\n",
    "    # å‡è®¾ load_arxiv_data å·²ç»åœ¨å‰é¢çš„ Cell å®šä¹‰å¥½äº†\n",
    "    arxiv_input, arxiv_linear, arxiv_dnn, _ = load_arxiv_data(ARXIV_CSV, ARXIV_VEC)\n",
    "    \n",
    "    # åˆå§‹åŒ–æ¨¡å‹ç»“æ„ (æ³¨æ„ hidden_units è¦å’Œè®­ç»ƒæ—¶ä¸€è‡´)\n",
    "    model_arxiv = DeepFM(arxiv_linear, arxiv_dnn, task='binary', dnn_hidden_units=(256, 128), device=DEVICE)\n",
    "    \n",
    "    # åŠ è½½æƒé‡\n",
    "    model_arxiv.load_state_dict(torch.load('deepfm_arxiv_weights.pth', map_location=DEVICE))\n",
    "    model_arxiv.eval()\n",
    "    print(\"âœ… ArXiv æ¨¡å‹åŠ è½½æˆåŠŸï¼Ready to serve.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ArXiv æ¨¡å‹åŠ è½½å¤±è´¥: {e}\")\n",
    "\n",
    "# ================= 2. å®šä¹‰ Flask åº”ç”¨ =================\n",
    "app_arxiv = Flask(__name__)\n",
    "\n",
    "def get_arxiv_recs(top_k=3):\n",
    "    # 1. é¢„æµ‹\n",
    "    pred_scores = model_arxiv.predict(arxiv_input, batch_size=256)\n",
    "    \n",
    "    # 2. è¯»å–å…ƒæ•°æ®\n",
    "    df = pd.read_csv(ARXIV_CSV)\n",
    "    df['score'] = pred_scores\n",
    "    \n",
    "    # 3. æ’åº\n",
    "    top_items = df.sort_values(by='score', ascending=False).head(top_k)\n",
    "    \n",
    "    # 4. æ ¼å¼åŒ–\n",
    "    results = []\n",
    "    for _, row in top_items.iterrows():\n",
    "        results.append({\n",
    "            \"title\": row['title'],\n",
    "            \"score\": float(row['score']),\n",
    "            \"type\": str(row.get('category', 'Unknown')),\n",
    "            # è¿™é‡Œå– pdf_url ä½œä¸º cover å­—æ®µè¿”å›ï¼Œæ–¹ä¾¿å‰ç«¯å¤„ç†\n",
    "            \"cover\": str(row.get('pdf_url', '')) \n",
    "        })\n",
    "    return results\n",
    "\n",
    "@app_arxiv.route('/recommend', methods=['POST'])\n",
    "def recommend_arxiv():\n",
    "    try:\n",
    "        req = request.json\n",
    "        top_k = req.get('top_k', 3)\n",
    "        print(f\"ğŸ“„ [ArXiv] æ”¶åˆ°è¯·æ±‚, Top {top_k}\")\n",
    "        \n",
    "        data = get_arxiv_recs(top_k)\n",
    "        return jsonify({\"status\": \"success\", \"service\": \"arxiv\", \"recommendations\": data})\n",
    "    except Exception as e:\n",
    "        return jsonify({\"status\": \"error\", \"message\": str(e)}), 500\n",
    "\n",
    "# ================= 3. å¯åŠ¨æœåŠ¡ =================\n",
    "def start_arxiv_server():\n",
    "    ngrok.set_auth_token(NGROK_TOKEN)\n",
    "    ngrok.kill() # æ€æ‰æ—§éš§é“\n",
    "    \n",
    "    public_url = ngrok.connect(PORT, bind_tls=True).public_url\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"ğŸ“„ ArXiv æœåŠ¡å·²å¯åŠ¨ï¼API åœ°å€: {public_url}/recommend\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    app_arxiv.run(port=PORT, use_reloader=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    t = threading.Thread(target=start_arxiv_server)\n",
    "    t.daemon = True\n",
    "    t.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
