{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfde963d",
   "metadata": {},
   "source": [
    "## ArXiv æ•°æ®åŠ è½½ä¸é¢„å¤„ç†\n",
    "\n",
    "æ ¸å¿ƒé€»è¾‘ï¼š\n",
    "\n",
    "1. å¤„ç† abstract + title: ä½¿ç”¨ sentence-transformers å®æ—¶ç”Ÿæˆ 384ç»´ è¯­ä¹‰å‘é‡ã€‚\n",
    "\n",
    "2. å¤„ç† category: ç®€å•çš„ Label Encodingã€‚\n",
    "\n",
    "3. é‡ç‚¹ï¼šå°† 384ç»´ å‘é‡æ‹†è§£ä¸º 384 ä¸ª DenseFeat å–‚ç»™æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6cfc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# ğŸ“„ ArXiv æ•°æ®åŠ è½½å™¨ (å«æ–‡æœ¬å‘é‡åŒ–)\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from deepctr_torch.inputs import SparseFeat, DenseFeat\n",
    "\n",
    "# å°è¯•å¯¼å…¥ sentence_transformersï¼Œå¦‚æœæ²¡æœ‰åˆ™ç”¨éšæœºå‘é‡ä»£æ›¿ï¼ˆé˜²æ­¢æŠ¥é”™ï¼‰\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    HAS_BERT = True\n",
    "except ImportError:\n",
    "    HAS_BERT = False\n",
    "    print(\"âš ï¸ æœªå®‰è£… sentence-transformersï¼Œå°†ä½¿ç”¨éšæœºå‘é‡æ¨¡æ‹Ÿæ•°æ®ã€‚\")\n",
    "\n",
    "def load_arxiv_data(csv_path):\n",
    "    print(f\"Loading ArXiv data from: {csv_path} ...\")\n",
    "    data = pd.read_csv(csv_path)\n",
    "    \n",
    "    # 1. åŸºç¡€ç‰¹å¾ç¼–ç \n",
    "    # Item ID (String -> Int)\n",
    "    lbe_id = LabelEncoder()\n",
    "    data['item_id_idx'] = lbe_id.fit_transform(data['item_id'])\n",
    "    \n",
    "    # Category (cs.CV -> 0, cs.RO -> 1)\n",
    "    lbe_cat = LabelEncoder()\n",
    "    data['category_idx'] = lbe_cat.fit_transform(data['category'])\n",
    "    \n",
    "    # 2. æ–‡æœ¬å‘é‡åŒ– (Text Embedding)\n",
    "    # æˆ‘ä»¬ä½¿ç”¨ Title + Abstract æ¥ç”Ÿæˆè¯­ä¹‰å‘é‡\n",
    "    EMBEDDING_DIM = 384 # MiniLM çš„æ ‡å‡†ç»´åº¦\n",
    "    \n",
    "    if HAS_BERT:\n",
    "        print(\"â³ æ­£åœ¨è°ƒç”¨ BERT æ¨¡å‹ç”Ÿæˆæ‘˜è¦å‘é‡ (è¿™å¯èƒ½éœ€è¦å‡ ç§’é’Ÿ)...\")\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        # æ‹¼æ¥ Title å’Œ Abstract å¢åŠ è¯­ä¹‰ä¸°å¯Œåº¦\n",
    "        sentences = (data['title'] + \". \" + data['abstract']).tolist()\n",
    "        text_vectors = model.encode(sentences) # Shape: (N, 384)\n",
    "    else:\n",
    "        # æ¨¡æ‹Ÿæ•°æ® (ä»…è°ƒè¯•ç”¨)\n",
    "        text_vectors = np.random.random((len(data), EMBEDDING_DIM)).astype(np.float32)\n",
    "\n",
    "    # 3. å®šä¹‰ç‰¹å¾é…ç½®\n",
    "    # Sparse éƒ¨åˆ†: ID å’Œ Category\n",
    "    feature_columns = [\n",
    "        SparseFeat(name='item_id_idx', vocabulary_size=data['item_id_idx'].max() + 1, embedding_dim=16),\n",
    "        SparseFeat(name='category_idx', vocabulary_size=data['category_idx'].max() + 1, embedding_dim=16)\n",
    "    ]\n",
    "    \n",
    "    # Dense éƒ¨åˆ†: æŠŠ 384 ç»´å‘é‡æ‹†æˆ 384 ä¸ª DenseFeat\n",
    "    # è¿™æ˜¯è®© DeepFM å¤„ç† BERT å‘é‡çš„æ ‡å‡†åšæ³•\n",
    "    for i in range(EMBEDDING_DIM):\n",
    "        feature_columns.append(DenseFeat(name=f'v_{i}', dimension=1))\n",
    "        \n",
    "    dnn_feature_columns = feature_columns\n",
    "    linear_feature_columns = feature_columns\n",
    "\n",
    "    # 4. ç»„è£…è¾“å…¥å­—å…¸\n",
    "    model_input = {\n",
    "        'item_id_idx': data['item_id_idx'].values,\n",
    "        'category_idx': data['category_idx'].values\n",
    "    }\n",
    "    \n",
    "    # æŠŠå‘é‡æ‹†å¼€å¡è¿›å­—å…¸ (v_0, v_1 ... v_383)\n",
    "    for i in range(EMBEDDING_DIM):\n",
    "        model_input[f'v_{i}'] = text_vectors[:, i]\n",
    "        \n",
    "    # Label\n",
    "    target = data['label'].values\n",
    "    \n",
    "    return model_input, linear_feature_columns, dnn_feature_columns, target\n",
    "\n",
    "# --- æµ‹è¯•è¿è¡Œ ---\n",
    "# arxiv_input, arxiv_linear, arxiv_dnn, arxiv_y = load_arxiv_data('arxiv_data.csv')\n",
    "# print(\"ArXiv Input Keys Count:\", len(arxiv_input)) # åº”è¯¥æ˜¯ 2 + 384 = 386 ä¸ª key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffce771",
   "metadata": {},
   "source": [
    "## ArXiv æ¨¡å‹è®­ç»ƒ (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea23499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from deepctr_torch.models import DeepFM\n",
    "from deepctr_torch.callbacks import EarlyStopping\n",
    "\n",
    "# 1. å‡†å¤‡æ•°æ®\n",
    "# ç¡®ä¿ä½ å·²ç»è¿è¡Œäº† Cell 2 å®šä¹‰äº† load_arxiv_data\n",
    "arxiv_csv_path = '../data/arxiv/train_arxiv_Researcher_LLM.csv'\n",
    "arxiv_vec_path = 'arxiv_vectors.npy'\n",
    "input_dict_arxiv, linear_cols_arxiv, dnn_cols_arxiv, target_arxiv = load_arxiv_data(arxiv_csv_path)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 2. åˆå§‹åŒ– DeepFM æ¨¡å‹\n",
    "# ArXiv çš„ç‰¹å¾ç»´åº¦å¾ˆå¤§ (384ç»´)ï¼Œå»ºè®®æŠŠ DNN å±‚è®¾å®½ä¸€ç‚¹\n",
    "model_arxiv = DeepFM(linear_feature_columns=linear_cols_arxiv, \n",
    "                     dnn_feature_columns=dnn_cols_arxiv, \n",
    "                     task='binary', \n",
    "                     dnn_hidden_units=(256, 128), \n",
    "                     device=device)\n",
    "\n",
    "# 3. ç¼–è¯‘ä¸è®­ç»ƒ\n",
    "model_arxiv.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy', 'auc'])\n",
    "\n",
    "history_arxiv = model_arxiv.fit(input_dict_arxiv, target_arxiv, \n",
    "                                batch_size=32, \n",
    "                                epochs=10, \n",
    "                                verbose=2, \n",
    "                                validation_split=0.2)\n",
    "\n",
    "print(\"ğŸ‰ ArXiv æ¨¡å‹è®­ç»ƒå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ed187f",
   "metadata": {},
   "source": [
    "## æ¨¡å‹ä¿å­˜ (Model Saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ddfae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# ğŸ’¾ æ¨¡å‹æƒé‡ä¿å­˜\n",
    "# ==========================================\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# 1. å®šä¹‰ä¿å­˜è·¯å¾„ (å»ºè®®æ”¾åœ¨å½“å‰å·¥ä½œç›®å½•)\n",
    "arxiv_model_path = 'deepfm_arxiv_weights.pth'\n",
    "\n",
    "# 2. ä¿å­˜ ArXiv æ¨¡å‹\n",
    "if 'model_arxiv' in locals():\n",
    "    torch.save(model_arxiv.state_dict(), arxiv_model_path)\n",
    "    print(f\"âœ… ArXiv æ¨¡å‹æƒé‡å·²ä¿å­˜è‡³: {arxiv_model_path}\")\n",
    "else:\n",
    "    print(\"âš ï¸ æœªæ£€æµ‹åˆ° model_arxivï¼Œè·³è¿‡ä¿å­˜ã€‚\")\n",
    "\n",
    "print(\"\\næç¤º: åœ¨ ModelScope å·¦ä¾§æ–‡ä»¶åˆ—è¡¨ä¸­åº”è¯¥èƒ½çœ‹åˆ°è¿™ä¸¤ä¸ª .pth æ–‡ä»¶äº†ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2f126",
   "metadata": {},
   "source": [
    "## åŠ è½½æƒé‡å¹¶å¯åŠ¨ API æœåŠ¡ (Loading & Serving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333381fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# ğŸš€ æ¨¡å‹åŠ è½½ä¸ API æœåŠ¡å¯åŠ¨\n",
    "# ==========================================\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from deepctr_torch.models import DeepFM\n",
    "from flask import Flask, request, jsonify\n",
    "import threading\n",
    "\n",
    "# 1. é‡æ–°å®šä¹‰ä¸€äº›åŸºç¡€é…ç½® (é˜²æ­¢é‡å¯å†…æ ¸åå˜é‡ä¸¢å¤±)\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ARXIV_CSV = '../data/arxiv/train_arxiv_Researcher_LLM.csv'  # ç¡®ä¿æ–‡ä»¶åå¯¹\n",
    "ARXIV_VEC = 'arxiv_vectors.npy'\n",
    "\n",
    "# 2. å®šä¹‰åŠ è½½å‡½æ•° (å¤ç”¨ä¹‹å‰çš„é€»è¾‘ï¼Œåªä¸ºäº†è·å–ç‰¹å¾åˆ—é…ç½®æ¥åˆå§‹åŒ–æ¨¡å‹)\n",
    "#    æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬éœ€è¦ import ä¹‹å‰å®šä¹‰çš„ load_arxiv_data\n",
    "#    å¦‚æœä½ é‡å¯äº†å†…æ ¸ï¼Œè¯·ç¡®ä¿æŠŠâ€œæ•°æ®é¢„å¤„ç†â€é‚£ä¸ª Cell å…ˆè¿è¡Œä¸€éï¼Œè®©å‡½æ•°ç”Ÿæ•ˆã€‚\n",
    "\n",
    "print(\"ğŸ”„ æ­£åœ¨åˆå§‹åŒ–æœåŠ¡ï¼Œé‡æ–°æ„å»ºæ¨¡å‹ç»“æ„...\")\n",
    "\n",
    "# --- B. åŠ è½½ ArXiv æ¨¡å‹ ---\n",
    "arxiv_input, arxiv_linear, arxiv_dnn, _ = load_arxiv_data(ARXIV_CSV, ARXIV_VEC)\n",
    "service_model_arxiv = DeepFM(arxiv_linear, arxiv_dnn, task='binary', dnn_hidden_units=(256, 128), device=DEVICE)\n",
    "try:\n",
    "    service_model_arxiv.load_state_dict(torch.load('deepfm_arxiv_weights.pth', map_location=DEVICE))\n",
    "    service_model_arxiv.eval()\n",
    "    print(\"âœ… ArXiv æ¨¡å‹åŠ è½½æˆåŠŸï¼\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ æœªæ‰¾åˆ° ArXiv æƒé‡æ–‡ä»¶ã€‚\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. æ­å»º Flask API\n",
    "# ==========================================\n",
    "app = Flask(__name__)\n",
    "\n",
    "# å®šä¹‰æ¨èé€»è¾‘ (å°è£…å¥½çš„å·¥å…·å‡½æ•°)\n",
    "def get_recommendation(model, input_data, csv_path, top_k=3):\n",
    "    # 1. é¢„æµ‹\n",
    "    pred_ans = model.predict(input_data, batch_size=256)\n",
    "    # 2. è¯»åŸå§‹æ–‡ä»¶ç”¨äºå±•ç¤º\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['score'] = pred_ans\n",
    "    # 3. æ’åº\n",
    "    top_items = df.sort_values(by='score', ascending=False).head(top_k)\n",
    "    # 4. æ ¼å¼åŒ–è¾“å‡º\n",
    "    results = []\n",
    "    for _, row in top_items.iterrows():\n",
    "        results.append({\n",
    "            \"id\": str(row['item_id']),\n",
    "            \"title\": row['title'],\n",
    "            \"score\": float(row['score']),\n",
    "            \"cover\": row.get('cover_url', row.get('pdf_url', '')),\n",
    "            \"tags\": row.get('tag_names', row.get('category', ''))\n",
    "        })\n",
    "    return results\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    return jsonify({\"status\": \"ok\", \"device\": DEVICE})\n",
    "\n",
    "@app.route('/recommend', methods=['POST'])\n",
    "def recommend():\n",
    "    \"\"\"\n",
    "    API æ¥å£\n",
    "    Input JSON: { \"type\": \"arxiv\", \"top_k\": 3 }\n",
    "    \"\"\"\n",
    "    data = request.json\n",
    "    rec_type = data.get('type', 'arxiv')\n",
    "    top_k = data.get('top_k', 3)\n",
    "    \n",
    "    try:\n",
    "        if rec_type == 'arxiv':\n",
    "            res = get_recommendation(service_model_arxiv, arxiv_input, ARXIV_CSV, top_k)\n",
    "        else:\n",
    "            return jsonify({\"error\": \"Unknown type\"}), 400\n",
    "            \n",
    "        return jsonify({\n",
    "            \"code\": 200,\n",
    "            \"type\": rec_type,\n",
    "            \"data\": res\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return jsonify({\"code\": 500, \"error\": str(e)})\n",
    "\n",
    "# ==========================================\n",
    "# 4. å¯åŠ¨æœåŠ¡ (åœ¨ Notebook ä¸­åå°è¿è¡Œ)\n",
    "# ==========================================\n",
    "def run_app():\n",
    "    # ç«¯å£è®¾ç½®ä¸º 6006 æˆ– 5000\n",
    "    app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)\n",
    "\n",
    "# ä½¿ç”¨çº¿ç¨‹å¯åŠ¨ï¼Œé¿å…é˜»å¡ Notebook çš„ä¸»è¿›ç¨‹ï¼Œè¿™æ ·ä½ è¿˜èƒ½ç»§ç»­è·‘ä¸‹é¢çš„æµ‹è¯•ä»£ç \n",
    "t = threading.Thread(target=run_app)\n",
    "t.daemon = True\n",
    "t.start()\n",
    "\n",
    "print(\"ğŸš€ æœåŠ¡å·²å¯åŠ¨ï¼ç›‘å¬ç«¯å£ 5000...\")\n",
    "print(\"ä½ å¯ä»¥è¿è¡Œä¸‹ä¸€ä¸ª Cell æ¥æµ‹è¯•æ¥å£ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38303ced",
   "metadata": {},
   "source": [
    "## æµ‹è¯•æœåŠ¡ (Client Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55aaef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# ğŸ§ª æ¥å£æµ‹è¯• (æ¨¡æ‹Ÿ Dify è°ƒç”¨)\n",
    "# ==========================================\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# ç­‰å¾…ä¸€ç§’è®©æœåŠ¡å®Œå…¨å¯åŠ¨\n",
    "time.sleep(2)\n",
    "\n",
    "url = \"http://127.0.0.1:5000/recommend\"\n",
    "\n",
    "# 2. æµ‹è¯• ArXiv æ¨è\n",
    "print(\"ğŸ“¡ è¯·æ±‚ ArXiv æ¨è...\")\n",
    "payload_arxiv = {\"type\": \"arxiv\", \"top_k\": 2}\n",
    "try:\n",
    "    resp = requests.post(url, json=payload_arxiv)\n",
    "    print(\"ArXiv Response:\", resp.json())\n",
    "except Exception as e:\n",
    "    print(\"ArXiv Request Failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6e21a5",
   "metadata": {},
   "source": [
    "## æœ€ç»ˆæœåŠ¡ä»£ç  Cell (ç›´æ¥è¿è¡Œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bdfca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# ğŸ“„ [Cell B] ArXiv DeepFM Server (Port 5001)\n",
    "# ==========================================\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from deepctr_torch.models import DeepFM\n",
    "from flask import Flask, request, jsonify\n",
    "from pyngrok import ngrok\n",
    "import threading\n",
    "\n",
    "# âš ï¸âš ï¸âš ï¸ å¡«å…¥ä½ çš„ Ngrok Token âš ï¸âš ï¸âš ï¸\n",
    "NGROK_TOKEN = \"è¿™é‡Œç²˜è´´ä½ çš„_Ngrok_Token\" \n",
    "PORT = 5001 # ArXiv æœåŠ¡è¿è¡Œåœ¨ 5001 ç«¯å£ (é˜²æ­¢å’Œ Steam å†²çª)\n",
    "\n",
    "# ================= 1. åŠ è½½ ArXiv æ¨¡å‹ =================\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ARXIV_CSV = 'train_arxiv_Researcher_LLM.csv'\n",
    "ARXIV_VEC = 'arxiv_vectors.npy'\n",
    "\n",
    "print(\"ğŸ”„ æ­£åœ¨åˆå§‹åŒ– ArXiv æ¨¡å‹...\")\n",
    "\n",
    "try:\n",
    "    # å‡è®¾ load_arxiv_data å·²ç»åœ¨å‰é¢çš„ Cell å®šä¹‰å¥½äº†\n",
    "    arxiv_input, arxiv_linear, arxiv_dnn, _ = load_arxiv_data(ARXIV_CSV, ARXIV_VEC)\n",
    "    \n",
    "    # åˆå§‹åŒ–æ¨¡å‹ç»“æ„ (æ³¨æ„ hidden_units è¦å’Œè®­ç»ƒæ—¶ä¸€è‡´)\n",
    "    model_arxiv = DeepFM(arxiv_linear, arxiv_dnn, task='binary', dnn_hidden_units=(256, 128), device=DEVICE)\n",
    "    \n",
    "    # åŠ è½½æƒé‡\n",
    "    model_arxiv.load_state_dict(torch.load('deepfm_arxiv_weights.pth', map_location=DEVICE))\n",
    "    model_arxiv.eval()\n",
    "    print(\"âœ… ArXiv æ¨¡å‹åŠ è½½æˆåŠŸï¼Ready to serve.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ArXiv æ¨¡å‹åŠ è½½å¤±è´¥: {e}\")\n",
    "\n",
    "# ================= 2. å®šä¹‰ Flask åº”ç”¨ =================\n",
    "app_arxiv = Flask(__name__)\n",
    "\n",
    "def get_arxiv_recs(top_k=3):\n",
    "    # 1. é¢„æµ‹\n",
    "    pred_scores = model_arxiv.predict(arxiv_input, batch_size=256)\n",
    "    \n",
    "    # 2. è¯»å–å…ƒæ•°æ®\n",
    "    df = pd.read_csv(ARXIV_CSV)\n",
    "    df['score'] = pred_scores\n",
    "    \n",
    "    # 3. æ’åº\n",
    "    top_items = df.sort_values(by='score', ascending=False).head(top_k)\n",
    "    \n",
    "    # 4. æ ¼å¼åŒ–\n",
    "    results = []\n",
    "    for _, row in top_items.iterrows():\n",
    "        results.append({\n",
    "            \"title\": row['title'],\n",
    "            \"score\": float(row['score']),\n",
    "            \"type\": str(row.get('category', 'Unknown')),\n",
    "            # è¿™é‡Œå– pdf_url ä½œä¸º cover å­—æ®µè¿”å›ï¼Œæ–¹ä¾¿å‰ç«¯å¤„ç†\n",
    "            \"cover\": str(row.get('pdf_url', '')) \n",
    "        })\n",
    "    return results\n",
    "\n",
    "@app_arxiv.route('/recommend', methods=['POST'])\n",
    "def recommend_arxiv():\n",
    "    try:\n",
    "        req = request.json\n",
    "        top_k = req.get('top_k', 3)\n",
    "        print(f\"ğŸ“„ [ArXiv] æ”¶åˆ°è¯·æ±‚, Top {top_k}\")\n",
    "        \n",
    "        data = get_arxiv_recs(top_k)\n",
    "        return jsonify({\"status\": \"success\", \"service\": \"arxiv\", \"recommendations\": data})\n",
    "    except Exception as e:\n",
    "        return jsonify({\"status\": \"error\", \"message\": str(e)}), 500\n",
    "\n",
    "# ================= 3. å¯åŠ¨æœåŠ¡ =================\n",
    "def start_arxiv_server():\n",
    "    ngrok.set_auth_token(NGROK_TOKEN)\n",
    "    ngrok.kill() # æ€æ‰æ—§éš§é“\n",
    "    \n",
    "    public_url = ngrok.connect(PORT, bind_tls=True).public_url\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"ğŸ“„ ArXiv æœåŠ¡å·²å¯åŠ¨ï¼API åœ°å€: {public_url}/recommend\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    app_arxiv.run(port=PORT, use_reloader=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    t = threading.Thread(target=start_arxiv_server)\n",
    "    t.daemon = True\n",
    "    t.start()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
