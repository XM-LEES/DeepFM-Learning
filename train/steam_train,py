import torch
import pandas as pd
import numpy as np
import ast
import threading
import time
from flask import Flask, request, jsonify
from pyngrok import ngrok
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from deepctr_torch.inputs import SparseFeat, DenseFeat, VarLenSparseFeat
from deepctr_torch.models import DeepFM

# ==========================================
# âš™ï¸ æœåŠ¡ç«¯é…ç½® (ä¸è®­ç»ƒæ—¶çš„æœ€ä½³å®è·µä¿æŒä¸€è‡´)
# ==========================================
class ServiceConfig:
    # âš ï¸ è¯·ç¡®ä¿è¿™é‡ŒæŒ‡å‘ä½ çœŸå®çš„è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„
    CSV_PATH = '../data/steam/deepfm_train_100k.csv' 
    MODEL_PATH = 'deepfm_steam_weights.pth'
    
    # --- æ ¸å¿ƒå‚æ•° (å¿…é¡»ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´) ---
    MAX_TAG_LEN = 5
    EMBEDDING_DIM = 32           # ä¹‹å‰è¯´çš„ 32 ç»´
    DNN_HIDDEN_UNITS = (128, 64) # âœ… ä¿®æ­£åçš„å°ç½‘ç»œï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
    DNN_DROPOUT = 0.5            # âœ… 0.5 Dropout
    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # --- æœåŠ¡å‚æ•° ---
    PORT = 5000
    NGROK_TOKEN = "è¿™é‡Œç²˜è´´ä½ çš„_Ngrok_Token"  # âš ï¸âš ï¸âš ï¸ è®°å¾—å¡« Token

cfg = ServiceConfig()

# ==========================================
# ğŸ› ï¸ æ•°æ®å¤„ç†å·¥å…·å‡½æ•°
# ==========================================
def pad_sequences(sequences, maxlen, value=0):
    """åºåˆ—è¡¥é½å·¥å…·"""
    result = np.full((len(sequences), maxlen), value, dtype=np.int32)
    for i, seq in enumerate(sequences):
        if len(seq) > 0:
            trunc = seq[:maxlen]
            result[i, :len(trunc)] = trunc
    return result

def load_data_and_build_model_struct(csv_path, config):
    """
    è¯»å–æ•°æ®ä»¥è·å–ç‰¹å¾ç»“æ„ (Feature Columns)ï¼Œ
    åŒæ—¶è¿”å›å…¨é‡æ•°æ® DataFrame ç”¨äºåœ¨çº¿æ£€ç´¢
    """
    print(f"ğŸ”„ è¯»å–æ•°æ®å¹¶æ„å»ºç‰¹å¾ç´¢å¼•: {csv_path} ...")
    try:
        data = pd.read_csv(csv_path)
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {csv_path}")
        return None, None, None

    # 1. è§£æ Tag å­—ç¬¦ä¸²
    data['tags_list'] = data['tags_list'].apply(lambda x: ast.literal_eval(x))
    
    # 2. Tag ç¼–ç  (æ˜ å°„ä¸º ID)
    all_tags = [tag for sublist in data['tags_list'] for tag in sublist]
    tag_lbe = LabelEncoder()
    tag_lbe.fit(all_tags)
    max_tag_id = len(tag_lbe.classes_) + 1
    
    # å°†åŸå§‹ Tag åˆ—è¡¨è½¬æ¢ä¸º ID åˆ—è¡¨
    data['tags_list_idx'] = data['tags_list'].apply(
        lambda x: [i + 1 for i in tag_lbe.transform(x)] if len(x) > 0 else []
    )
    
    # 3. Item ç¼–ç 
    item_lbe = LabelEncoder()
    data['item_id_idx'] = item_lbe.fit_transform(data['item_id'])
    max_item_id = data['item_id_idx'].max() + 1
    
    # 4. ä»·æ ¼å½’ä¸€åŒ–
    mms = MinMaxScaler(feature_range=(0, 1))
    data['price_norm'] = mms.fit_transform(data[['price']])
    
    # 5. å®šä¹‰ç‰¹å¾åˆ— (Feature Columns)
    fixlen_feature_columns = [
        SparseFeat('item_id_idx', vocabulary_size=max_item_id, embedding_dim=config.EMBEDDING_DIM),
        DenseFeat('price_norm', dimension=1)
    ]
    
    varlen_feature_columns = [
        VarLenSparseFeat(
            SparseFeat('tags', vocabulary_size=max_tag_id, embedding_dim=config.EMBEDDING_DIM),
            maxlen=config.MAX_TAG_LEN, combiner='mean', length_name=None
        )
    ]
    
    linear_feature_columns = fixlen_feature_columns + varlen_feature_columns
    dnn_feature_columns = fixlen_feature_columns + varlen_feature_columns
    
    return linear_feature_columns, dnn_feature_columns, data

# ==========================================
# ğŸš€ Flask æœåŠ¡é€»è¾‘
# ==========================================
app = Flask(__name__)
model_steam = None
full_data_df = None

def init_model():
    """åˆå§‹åŒ–æ¨¡å‹å¹¶åŠ è½½æƒé‡"""
    global model_steam, full_data_df
    
    linear_cols, dnn_cols, df = load_data_and_build_model_struct(cfg.CSV_PATH, cfg)
    
    if linear_cols is None:
        return

    full_data_df = df # ç¼“å­˜å…¨é‡æ•°æ®
    
    # åˆå§‹åŒ–æ¨¡å‹éª¨æ¶
    print(f"ğŸ”§ åˆå§‹åŒ– DeepFM (DNN: {cfg.DNN_HIDDEN_UNITS}, Embed: {cfg.EMBEDDING_DIM})...")
    model_steam = DeepFM(linear_feature_columns=linear_cols, 
                         dnn_feature_columns=dnn_cols, 
                         task='binary', 
                         dnn_hidden_units=cfg.DNN_HIDDEN_UNITS, 
                         dnn_dropout=cfg.DNN_DROPOUT,
                         device=cfg.DEVICE)
    
    # åŠ è½½æƒé‡
    try:
        state_dict = torch.load(cfg.MODEL_PATH, map_location=cfg.DEVICE)
        model_steam.load_state_dict(state_dict)
        model_steam.eval()
        print("âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸï¼æœåŠ¡å·²å°±ç»ªã€‚")
    except Exception as e:
        print(f"âŒ æƒé‡åŠ è½½å¤±è´¥: {e}")
        print("âš ï¸ æç¤º: è¯·å…ˆè¿è¡Œ Notebook å®Œæˆè®­ç»ƒï¼Œå¹¶ç¡®ä¿ DNN å‚æ•°ä¸€è‡´ã€‚")
        model_steam = None

@app.route('/recommend', methods=['POST'])
def recommend():
    if model_steam is None:
        return jsonify({"code": 500, "error": "Model not initialized"}), 500
        
    try:
        req = request.json
        top_k = req.get('top_k', 3)
        print(f"ğŸ® [Steam] æ”¶åˆ°æ¨èè¯·æ±‚: Top {top_k}")
        
        # --- ç®€å•å…¨é‡é‡æ’é€»è¾‘ ---
        # 1. æ„é€ å…¨é‡è¾“å…¥
        tags_padded = pad_sequences(list(full_data_df['tags_list_idx']), maxlen=cfg.MAX_TAG_LEN, value=0)
        
        model_input = {
            'item_id_idx': full_data_df['item_id_idx'].values,
            'price_norm': full_data_df['price_norm'].values,
            'tags': tags_padded
        }
        
        # 2. æ‰¹é‡é¢„æµ‹
        with torch.no_grad():
            pred_scores = model_steam.predict(model_input, batch_size=4096)
            
        # 3. æ’åºå– TopK
        # ä½¿ç”¨ copy é¿å…æ±¡æŸ“åŸå§‹æ•°æ®
        res_df = full_data_df.copy()
        res_df['pred_score'] = pred_scores
        
        # æŒ‰åˆ†æ•°é™åºï¼Œå¹¶å¯¹ item_id å»é‡ (é˜²æ­¢åŒä¸€æ¸¸æˆå¤šæ¡æ•°æ®å¹²æ‰°)
        top_items = res_df.sort_values(by='pred_score', ascending=False).drop_duplicates(subset=['item_id']).head(top_k)
        
        # 4. æ ¼å¼åŒ–è¿”å›
        results = []
        for _, row in top_items.iterrows():
            results.append({
                "id": str(row['item_id']),
                "title": row['title'],
                "score": float(row['pred_score']),
                "tags": row.get('tag_names', ''), # å±•ç¤ºä¸­æ–‡ Tag
                "cover": row.get('cover_url', '') # å±•ç¤ºå°é¢å›¾
            })
            
        return jsonify({
            "code": 200, 
            "data": results,
            "message": "Success"
        })
        
    except Exception as e:
        print(f"Error: {e}")
        return jsonify({"code": 500, "error": str(e)}), 500

def start_server():
    # Ngrok å†…ç½‘ç©¿é€é…ç½®
    if not cfg.NGROK_TOKEN.startswith("è¿™é‡Œ"):
        ngrok.set_auth_token(cfg.NGROK_TOKEN)
        ngrok.kill()
        try:
            public_url = ngrok.connect(cfg.PORT, bind_tls=True).public_url
            print(f"\nğŸŒ å…¬ç½‘è®¿é—®åœ°å€: {public_url}/recommend\n")
        except Exception as e:
            print(f"Ngrok Error: {e}")
    else:
        print("âš ï¸ æœªé…ç½® Ngrok Tokenï¼Œä»…æ”¯æŒæœ¬åœ°è®¿é—® (http://127.0.0.1:5000)")

    app.run(host='0.0.0.0', port=cfg.PORT, use_reloader=False)

if __name__ == '__main__':
    # 1. åˆå§‹åŒ–
    init_model()
    # 2. å¯åŠ¨
    start_server()