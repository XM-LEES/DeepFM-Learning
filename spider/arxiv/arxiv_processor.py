import pandas as pd
import ast

# === ğŸ“ æ¨¡æ‹Ÿç§‘ç ”äººå‘˜ç”»åƒ ===
# åœºæ™¯ A: ä¸“æ³¨äºå¤§è¯­è¨€æ¨¡å‹ (LLM) å’Œ Agent çš„ç ”ç©¶ç”Ÿ
RESEARCHER_LLM = {
    "name": "Researcher_LLM",
    "interest_keywords": ["LLM", "Large Language Model", "Agent", "RAG", "Prompt", "Transformer", "Generative"],
    "ignore_keywords": ["Image", "Detection", "Segmentation", "3D", "Reinforcement Learning"] # å¯¹çº¯ CV ä¸æ„Ÿå…´è¶£
}

# åœºæ™¯ B: ä¸“æ³¨äºè®¡ç®—æœºè§†è§‰ (CV) çš„ç ”ç©¶ç”Ÿ
RESEARCHER_CV = {
    "name": "Researcher_CV",
    "interest_keywords": ["Vision", "Image", "Detection", "3D", "Segmentation", "Object", "Diffusion"],
    "ignore_keywords": ["Language", "Text", "NLP", "Audio"]
}

# åœºæ™¯ C: ç½‘ç»œå®‰å…¨ä¸å¯¹æŠ—æ”»å‡» (Security & Safety)
# ğŸ¯ ç‰¹ç‚¹ï¼šå–œæ¬¢æ‰¾æ¼æ´ã€é˜²å¾¡ã€éšç§ä¿æŠ¤ï¼Œå¯¹çº¯ç²¹çš„æ¨¡å‹æ¶æ„ä¼˜åŒ–ä¸æ„Ÿå…´è¶£
RESEARCHER_SEC = {
    "name": "Researcher_Security",
    "interest_keywords": ["Adversarial", "Attack", "Defense", "Privacy", "Security", "Backdoor", "Poisoning", "Robustness", "Safety"],
    "ignore_keywords": ["Gaming", "Art", "Music", "Recommendation", "UI/UX"] 
}

# åœºæ™¯ D: å›¾ç¥ç»ç½‘ç»œä¸æ¨èç®—æ³• (Graph & RecSys)
# ğŸ¯ ç‰¹ç‚¹ï¼šå–œæ¬¢ç¤¾äº¤ç½‘ç»œã€èŠ‚ç‚¹åˆ†ç±»ã€æ¨èç³»ç»Ÿï¼Œè¿™è·Ÿçº¯å›¾åƒæˆ–çº¯æ–‡æœ¬å¤„ç†å·®åˆ«å¾ˆå¤§
RESEARCHER_GRAPH = {
    "name": "Researcher_Graph",
    "interest_keywords": ["Graph", "GNN", "Node", "Link Prediction", "Social Network", "Recommendation", "Collaborative Filtering", "Contrastive Learning"],
    "ignore_keywords": ["Transformer", "Language", "Robot", "Hardware"] # ç¨å¾®æ’æ–¥çº¯è¯­è¨€æ¨¡å‹ï¼Œæ›´å…³æ³¨ç»“æ„åŒ–æ•°æ®
}

# åœºæ™¯ E: è¾¹ç¼˜è®¡ç®—ä¸æ¨¡å‹å‹ç¼© (Edge AI & Systems)
# ğŸ¯ ç‰¹ç‚¹ï¼šè¿™æ˜¯ä¸€ç§â€œå·¥ç¨‹æ´¾â€ç”»åƒã€‚ä»–ä»¬ä¸å…³å¿ƒæ¨¡å‹æœ‰å¤šèªæ˜ï¼Œåªå…³å¿ƒæ¨¡å‹æœ‰å¤šå¿«ã€å¤šå°ã€‚
RESEARCHER_SYS = {
    "name": "Researcher_System",
    "interest_keywords": ["Quantization", "Compression", "Pruning", "Edge", "Latency", "Efficient", "Mobile", "Hardware", "FPGA", "Accelerator"],
    "ignore_keywords": ["Theory", "Proof", "Survey", "Ethics"] # è®¨åŒçº¯ç†è®ºè¯æ˜æˆ–ä¼¦ç†è®¨è®º
}

# åœºæ™¯ F: å¤šæ¨¡æ€ä¸AIGC (Multimodal & AIGC)
# ğŸ¯ ç‰¹ç‚¹ï¼šä»€ä¹ˆéƒ½è¦â€”â€”æ—¢è¦å›¾åˆè¦æ–‡ï¼Œç”šè‡³è¦è§†é¢‘ã€‚è¿™æ˜¯å½“ä¸‹çš„â€œç¼åˆæ€ªâ€çƒ­ç‚¹ã€‚
RESEARCHER_MULTI = {
    "name": "Researcher_Multimodal",
    "interest_keywords": ["Multimodal", "Video", "Audio", "Text-to-Image", "Diffusion", "CLIP", "Alignment", "Cross-modal"],
    "ignore_keywords": ["Security", "Encryption", "Database", "Network"] # å¯¹åº•å±‚è®¾æ–½ä¸æ„Ÿå…´è¶£
}

def format_authors(authors_str):
    """æŠŠä½œè€…åˆ—è¡¨è½¬ä¸º 'Li et al.' æ ¼å¼ï¼Œçœç©ºé—´"""
    try:
        if isinstance(authors_str, str):
            authors = ast.literal_eval(authors_str)
        else:
            authors = authors_str
        
        if not authors: return "Unknown"
        if len(authors) > 1:
            # å–ç¬¬ä¸€ä¸ªä½œè€…çš„å§“ (ç®€å•å¤„ç†)
            first_name = authors[0].split(' ')[-1] 
            return f"{first_name} et al."
        else:
            return authors[0]
    except:
        return "Unknown"

def generate_academic_label(row, profile):
    """
    æ ¹æ®ã€æ ‡é¢˜+æ‘˜è¦ã€‘çš„å…³é”®è¯åŒ¹é…ç”Ÿæˆ Label
    """
    score = 0.5
    
    # æ‹¼æ¥æ ‡é¢˜å’Œæ‘˜è¦è¿›è¡Œæ£€ç´¢
    content = (str(row['title']) + " " + str(row['abstract'])).lower()
    
    # 1. å…´è¶£è¯åŒ¹é… (å‘½ä¸­åŠ åˆ†)
    for kw in profile['interest_keywords']:
        if kw.lower() in content:
            score += 0.3
            # åªè¦å‘½ä¸­ä¸€ä¸ªæ ¸å¿ƒè¯ï¼Œæ¦‚ç‡å°±å¾ˆå¤§
            break 
            
    # 2. å±è”½è¯åŒ¹é… (å‘½ä¸­å‡åˆ†)
    for kw in profile['ignore_keywords']:
        if kw.lower() in content:
            score -= 0.4
            
    # 3. å¼•ç”¨æ•°/çƒ­åº¦æ¨¡æ‹Ÿ (å¦‚æœæ²¡æœ‰çœŸå®å¼•ç”¨æ•°ï¼Œå¯ä»¥ç”¨å‘è¡¨æ—¶é—´æ¨¡æ‹Ÿï¼šè¶Šæ–°è¶Šå®¹æ˜“ç‚¹)
    # è¿™é‡Œç®€å•å‡è®¾ï¼šå¦‚æœæ˜¯ cs.AI æˆ– cs.CL ç±»åˆ«çš„ï¼ŒåŸºç¡€åˆ†é«˜ä¸€ç‚¹
    if row['category'] in ['cs.CL', 'cs.AI'] and "LLM" in profile['name']:
        score += 0.1
        
    return 1 if score >= 0.6 else 0

def process_arxiv(profile):
    print(f"âš™ï¸ [Step 2] æ­£åœ¨ä¸ºç”¨æˆ· [{profile['name']}] ç”Ÿæˆè®­ç»ƒæ•°æ®...")
    
    df = pd.read_csv("../../data/arxiv/arxiv_raw_data.csv")
    
    # 1. æ ¼å¼åŒ–ä½œè€… (ç»™ Dify çœ‹çš„)
    df['display_authors'] = df['authors_raw'].apply(format_authors)
    
    # 2. ç”Ÿæˆ Label (ç»™ DeepFM è®­ç»ƒç”¨çš„)
    df['label'] = df.apply(lambda row: generate_academic_label(row, profile), axis=1)
    
    # 3. å…³é”®ï¼šä¸º DeepFM å‡†å¤‡ Embedding æ¥å£
    # æ³¨æ„ï¼šDeepFM æ— æ³•ç›´æ¥è®­ç»ƒ string ç±»å‹çš„ abstractã€‚
    # åœ¨è¿™é‡Œæˆ‘ä»¬ä¸è¿›è¡Œ BERT è½¬æ¢ï¼ˆå¤ªæ…¢ï¼‰ï¼Œè€Œæ˜¯ä¿ç•™ textï¼Œ
    # å¹¶åœ¨ Dataset Loader é˜¶æ®µæˆ–è€…å•ç‹¬è„šæœ¬é‡Œåš text -> vector
    
    # å¯¼å‡º
    output_file = f"../../data/arxiv/train_arxiv_{profile['name']}.csv"
    cols = ['item_id', 'title', 'category', 'abstract', 'display_authors', 'published', 'pdf_url', 'label']
    df[cols].to_csv(output_file, index=False)
    
    print(f"   âœ… ç”Ÿæˆå®Œæ¯•: {output_file}")
    print(f"   ğŸ“Š æ­£æ ·æœ¬(æ„Ÿå…´è¶£)æ¯”ä¾‹: {df['label'].mean():.2%}\n")

if __name__ == "__main__":
# å®šä¹‰ä¸€ä¸ªç”»åƒåˆ—è¡¨
    profiles = [
        RESEARCHER_LLM, 
        RESEARCHER_CV, 
        RESEARCHER_SEC, 
        RESEARCHER_GRAPH, 
        RESEARCHER_SYS,
        RESEARCHER_MULTI
    ]

    # å¾ªç¯ç”Ÿæˆæ‰€æœ‰è®­ç»ƒé›†
    for p in profiles:
        process_arxiv(p)