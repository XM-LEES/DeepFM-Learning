import feedparser
import pandas as pd
import time
import urllib.parse

# === é…ç½® ===
# æœç´¢å…³é”®è¯ï¼šè¿™é‡Œæœç´¢ "Artificial Intelligence" ä¹Ÿå°±æ˜¯ cs.AI ç±»åˆ«
SEARCH_QUERY = "cat:cs.AI OR cat:cs.CL OR cat:cs.CV" 
MAX_RESULTS = 500 # å»ºè®® 200-500 æ¡
BASE_URL = "http://export.arxiv.org/api/query?"

def fetch_arxiv_raw():
    print(f"ğŸš€ [Step 1] å¼€å§‹çˆ¬å– ArXiv è®ºæ–‡ (Query: {SEARCH_QUERY})...")
    
    # ArXiv API æ”¯æŒä¸€æ¬¡æ€§è¯·æ±‚å¤§é‡æ•°æ®ï¼Œä¸éœ€è¦åƒ Steam é‚£æ ·ç¿»é¡µ
    # ä½†ä¸ºäº†ç¨³å®šæ€§ï¼Œå»ºè®®æ¯ 100 æ¡è¯·æ±‚ä¸€æ¬¡
    all_papers = []
    batch_size = 100
    
    for start in range(0, MAX_RESULTS, batch_size):
        print(f"   æ­£åœ¨è·å–ç¬¬ {start} - {start+batch_size} æ¡...")
        
        params = {
            "search_query": SEARCH_QUERY,
            "start": start,
            "max_results": batch_size,
            "sortBy": "submittedDate", # æŒ‰æäº¤æ—¶é—´å€’åº (æœ€æ–°çš„åœ¨å‰é¢)
            "sortOrder": "descending"
        }
        
        url = BASE_URL + urllib.parse.urlencode(params)
        
        # ä½¿ç”¨ feedparser è§£æ XML
        feed = feedparser.parse(url)
        
        if not feed.entries:
            print("   âš ï¸ æœªè·å–åˆ°æ•°æ®ï¼Œå¯èƒ½å·²è¾¾åˆ°æœ«å°¾ã€‚")
            break
            
        for entry in feed.entries:
            try:
                # 1. ID æ¸…æ´— (http://arxiv.org/abs/2312.00001v1 -> 2312.00001v1)
                paper_id = entry.id.split('/abs/')[-1]
                
                # 2. æå– PDF é“¾æ¥
                pdf_url = ""
                for link in entry.links:
                    if link.type == 'application/pdf':
                        pdf_url = link.href
                
                # 3. æå–ä¸»åˆ†ç±» (Primary Category)
                primary_cat = entry.arxiv_primary_category['term']
                
                # 4. æå–ä½œè€… (List)
                authors = [author.name for author in entry.authors]
                
                all_papers.append({
                    "item_id": paper_id,
                    "title": entry.title.replace('\n', ' '),
                    "abstract": entry.summary.replace('\n', ' '),
                    "authors_raw": authors,       # å­˜ä¸º List
                    "category": primary_cat,      # Sparse Feature
                    "published": entry.published[:10], # 2024-12-15
                    "pdf_url": pdf_url            # Meta (Dify è·³è½¬ç”¨)
                })
            except Exception as e:
                continue
        
        # âš ï¸ ArXiv API è§„å®šå¿…é¡»é—´éš” 3 ç§’
        time.sleep(3)

    # ä¿å­˜åŸå§‹æ•°æ®
    df = pd.DataFrame(all_papers)
    df.to_csv("../../data/arxiv/arxiv_raw_data.csv", index=False, encoding='utf-8-sig')
    print(f"âœ… [Step 1] å®Œæˆï¼åŸå§‹æ•°æ®å·²ä¿å­˜è‡³ '../../data/arxiv/arxiv_raw_data.csv' (å…± {len(df)} æ¡)")

if __name__ == "__main__":
    fetch_arxiv_raw()