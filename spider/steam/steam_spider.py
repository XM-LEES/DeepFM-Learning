import requests
from bs4 import BeautifulSoup
import pandas as pd
import json
import time
import random

# === é…ç½® ===
BASE_URL = "https://store.steampowered.com/search/"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Cookie": "birthtime=946684801; lastagecheckage=1-0-1900; wants_mature_content=1;"
}

def get_game_reviews(app_id):
    """
    è°ƒç”¨ Steam API è·å–è¯¥æ¸¸æˆæœ€çƒ­é—¨çš„ 5 æ¡ä¸­æ–‡è¯„è®º
    ç”¨äº SFT å¾®è°ƒç´ æ
    """
    # url = f"https://store.steampowered.com/appreviews/{app_id}?json=1&language=schinese&filter=summary&num_per_page=5"
    url = f"https://store.steampowered.com/appreviews/{app_id}?json=1&language=schinese&filter=summary"
    try:
        # è¿™é‡Œä¸éœ€è¦ cookie ä¹Ÿèƒ½è·‘ï¼Œå¦‚æœæŠ¥é”™å†åŠ  header
        resp = requests.get(url, timeout=5)
        if resp.status_code == 200:
            data = resp.json()
            if data['success'] == 1:
                reviews = []
                for r in data['reviews']:
                    # åªè¦çº¯æ–‡æœ¬ï¼Œå»æ‰å¤ªçŸ­çš„åƒåœ¾è¯„è®º
                    content = r['review'].strip()
                    if len(content) > 10: 
                        reviews.append(content)
                return reviews
    except:
        pass
    return []
    

def run_spider(max_pages=5):
    print(f"ğŸš€ [Step 1] å¼€å§‹çˆ¬å–åŸå§‹æ•°æ® (Raw Data)...")
    raw_games = []
    
    for page in range(1, max_pages + 1):
        print(f"   æ­£åœ¨ä¸‹è½½ç¬¬ {page} é¡µ...")
        params = {"filter": "topsellers", "page": page, "cc": "cn", "l": "schinese"}
        
        try:
            resp = requests.get(BASE_URL, params=params, headers=HEADERS, timeout=15)
            if resp.status_code != 200: continue
            
            soup = BeautifulSoup(resp.text, "html.parser")
            rows = soup.select("#search_resultsRows > a")
            
            for row in rows:
                try:
                    # è·å–æœ€åŸå§‹çš„æ•°æ®ï¼Œä¸æ¸…æ´—ï¼Œä¸æ‰“æ ‡
                    app_id = row.get("data-ds-appid")
                    if not app_id: continue
                    
                    title = row.select_one(".title").text.strip()
                    
                    # ä»·æ ¼ä¿ç•™åŸå§‹å­—ç¬¦ä¸² (å¦‚ "Â¥ 136.00" æˆ– "Free")ï¼Œç•™ç»™ç¬¬äºŒæ­¥å¤„ç†
                    price_div = row.select_one(".discount_final_price") or row.select_one(".search_price")
                    price_raw = price_div.text.strip() if price_div else "0"
                    
                    # Tags ä¿ç•™åŸå§‹ JSON åˆ—è¡¨
                    tag_str = row.get("data-ds-tagids")
                    tags_raw = json.loads(tag_str) if tag_str else []
                    
                    # å›¾ç‰‡ URL
                    img_tag = row.select_one("img")
                    img_url = img_tag.get('srcset', '').split(', ')[0].split(' ')[0] or img_tag.get('src')
                    
                    # å¥½è¯„ä¿¡æ¯ (ç”¨äºç¬¬äºŒæ­¥è¾…åŠ©æ‰“æ ‡)
                    review_sum = row.select_one(".search_review_summary")
                    review_raw = review_sum['data-tooltip-html'] if review_sum else ""

                    # ===  æ–°å¢ï¼šè·å–è¯„è®ºæ•°æ® ===
                    # åªæœ‰å½“æˆ‘ä»¬éœ€è¦ SFT ç´ ææ—¶æ‰è·‘è¿™ä¸ªï¼Œä¼šæ…¢ä¸€ç‚¹ç‚¹
                    reviews = get_game_reviews(app_id)

                    raw_games.append({
                        "item_id": app_id,
                        "title": title,
                        "price_raw": price_raw,
                        "tags_raw": tags_raw,      # å­˜ä¸º List
                        "review_raw": review_raw,  # å­˜åŸå§‹å¥½è¯„ HTML
                        "cover_url": img_url,
                        "user_reviews": reviews    # æŠŠçˆ¬åˆ°çš„è¯„è®ºå­˜æˆåˆ—è¡¨
                    })

                    print(f"   å·²è·å–: {title} (å« {len(reviews)} æ¡è¯„è®º)")
                    
                except Exception as e:
                    continue
            
            # ç¨å¾®å¤šç¡ä¸€ä¼šï¼Œå› ä¸ºå¤šè¯·æ±‚äº† API
            time.sleep(random.uniform(3, 6))
            
        except Exception as e:
            print(f"   âŒ Error: {e}")

    # ä¿å­˜åŸå§‹æ•°æ®
    df = pd.DataFrame(raw_games)
    # å¼ºåˆ¶æŠŠ tags å­˜ä¸ºå­—ç¬¦ä¸²å½¢å¼ï¼Œé¿å… CSV è¯»å–æ­§ä¹‰
    df['tags_raw'] = df['tags_raw'].apply(json.dumps)
    # å­˜ review æ—¶é˜²æ­¢ CSV é”™ä¹±ï¼Œå»ºè®®ç›´æ¥å­˜ JSON æ ¼å¼çš„å­—ç¬¦ä¸²
    df['user_reviews'] = df['user_reviews'].apply(json.dumps, ensure_ascii=False)
    df.to_csv("../../data/steam/steam_raw_data.csv", index=False, encoding='utf-8-sig')
    print(f"âœ… [Step 1] å®Œæˆï¼åŸå§‹æ•°æ®å·²ä¿å­˜è‡³ '../../data/steam/steam_raw_data.csv' (å…± {len(df)} æ¡)")

if __name__ == "__main__":
    run_spider(max_pages=60) # å»ºè®®çˆ¬ 5 é¡µï¼Œçº¦ 250 æ¡æ•°æ®
    # 25 * 60 = 1500 æ¡æ•°æ®