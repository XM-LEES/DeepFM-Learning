import requests
from bs4 import BeautifulSoup
import pandas as pd
import json
import time
import random

# === é…ç½® ===
BASE_URL = "https://store.steampowered.com/search/"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Cookie": "birthtime=946684801; lastagecheckage=1-0-1900; wants_mature_content=1;"
}

def run_spider(max_pages=5):
    print(f"ğŸš€ [Step 1] å¼€å§‹çˆ¬å–åŸå§‹æ•°æ® (Raw Data)...")
    raw_games = []
    
    for page in range(1, max_pages + 1):
        print(f"   æ­£åœ¨ä¸‹è½½ç¬¬ {page} é¡µ...")
        params = {"filter": "topsellers", "page": page, "cc": "cn", "l": "schinese"}
        
        try:
            resp = requests.get(BASE_URL, params=params, headers=HEADERS, timeout=15)
            if resp.status_code != 200: continue
            
            soup = BeautifulSoup(resp.text, "html.parser")
            rows = soup.select("#search_resultsRows > a")
            
            for row in rows:
                try:
                    # è·å–æœ€åŸå§‹çš„æ•°æ®ï¼Œä¸æ¸…æ´—ï¼Œä¸æ‰“æ ‡
                    app_id = row.get("data-ds-appid")
                    if not app_id: continue
                    
                    title = row.select_one(".title").text.strip()
                    
                    # ä»·æ ¼ä¿ç•™åŸå§‹å­—ç¬¦ä¸² (å¦‚ "Â¥ 136.00" æˆ– "Free")ï¼Œç•™ç»™ç¬¬äºŒæ­¥å¤„ç†
                    price_div = row.select_one(".discount_final_price") or row.select_one(".search_price")
                    price_raw = price_div.text.strip() if price_div else "0"
                    
                    # Tags ä¿ç•™åŸå§‹ JSON åˆ—è¡¨
                    tag_str = row.get("data-ds-tagids")
                    tags_raw = json.loads(tag_str) if tag_str else []
                    
                    # å›¾ç‰‡ URL
                    img_tag = row.select_one("img")
                    img_url = img_tag.get('srcset', '').split(', ')[0].split(' ')[0] or img_tag.get('src')
                    
                    # å¥½è¯„ä¿¡æ¯ (ç”¨äºç¬¬äºŒæ­¥è¾…åŠ©æ‰“æ ‡)
                    review_sum = row.select_one(".search_review_summary")
                    review_raw = review_sum['data-tooltip-html'] if review_sum else ""

                    raw_games.append({
                        "item_id": app_id,
                        "title": title,
                        "price_raw": price_raw,
                        "tags_raw": tags_raw,      # å­˜ä¸º List
                        "review_raw": review_raw,  # å­˜åŸå§‹å¥½è¯„ HTML
                        "cover_url": img_url
                    })
                except:
                    continue
            
            time.sleep(random.uniform(1, 3))
            
        except Exception as e:
            print(f"   âŒ Error: {e}")

    # ä¿å­˜åŸå§‹æ•°æ®
    df = pd.DataFrame(raw_games)
    # å¼ºåˆ¶æŠŠ tags å­˜ä¸ºå­—ç¬¦ä¸²å½¢å¼ï¼Œé¿å… CSV è¯»å–æ­§ä¹‰
    df['tags_raw'] = df['tags_raw'].apply(json.dumps)
    df.to_csv("../../data/steam/steam_raw_data.csv", index=False, encoding='utf-8-sig')
    print(f"âœ… [Step 1] å®Œæˆï¼åŸå§‹æ•°æ®å·²ä¿å­˜è‡³ '../../data/steam/steam_raw_data.csv' (å…± {len(df)} æ¡)")

if __name__ == "__main__":
    run_spider(max_pages=20) # å»ºè®®çˆ¬ 5 é¡µï¼Œçº¦ 250 æ¡æ•°æ®